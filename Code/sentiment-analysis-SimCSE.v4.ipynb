{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1679703770135,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"XQ8gf-7Ya1Hk","outputId":"3d696c39-cc95-48d5-8b28-33aa6792bc23"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat Mar 25 00:22:48 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0    44W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12815,"status":"ok","timestamp":1679703782941,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"mv4x_nCiHgMv","outputId":"2d9a74a2-15ef-424e-8007-e3626ae46101"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gShCZsI6H8SK"},"outputs":[],"source":["%%sh\n","mkdir machine-learning\n","rsync -av --progress /content/drive/MyDrive//machine-learning/* /content/machine-learning/ --exclude model-output/model_files/\n","# cp -r /content/drive/MyDrive//machine-learning/* /content/machine-learning/\n","# cp /content/drive/MyDrive/Arbor/machine-learning/brand/model-preparation/News_processed_031_060.csv /content/machine-learning/model-preparation/"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":375,"status":"ok","timestamp":1679708617712,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"YLf6Ls37QQvf"},"outputs":[],"source":["DATE = '2023_03_24_'\n","MODEL_RUN = 1\n","PREDICTION_RUN = '241950' "]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3347,"status":"ok","timestamp":1679704090641,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"9-AY2BFNIsVO"},"outputs":[],"source":["\n","train_file_name    = '05.Senti4SD_GoldStandard_EmotionPolarity-ML.csv'\n","additional_testing_file_name    = 'library-selection-factors.csv'\n","run_additional_testing = False\n","\n","#train_file_name    = \"BenchmarkUddinSO-ConsoliatedAspectSentiment-ML.csv\"\n","#train_file_name    = \"BenchmarkUddinSO-ConsoliatedAspectSentiment-ML-D.csv\"\n","#train_file_name    = \"BenchmarkUddinSO-ConsoliatedAspectSentiment-ML-O.csv\"\n","#train_file_name    = \"07.SO.BERT4SentiSE-ML.csv\"\n","#train_file_name    = \"07.SO.BERT4SentiSE.Combined-ML.csv\"\n","\n","\n","\n","topic_title = \"sentiment\"\n","logging_steps = 50\n","num_train_epochs = 2\n","\n","data_dir   = \"/content/machine-learning/model-preparation/\"\n","output_dir = \"/content/machine-learning/model-output/\"\n","\n","# for multiple model and dataset files\n","dataset_dir   = \"/content/machine-learning/model-preparation/0.Combined-Dataset/\"\n","model_output_dir_base = \"/content/machine-learning/model-output/model_files/\"\n","prediction_output_dir = \"/content/machine-learning/model-output/prediction_files/\"\n","\n","train_file_path = data_dir+train_file_name\n","\n","#predict_file_name = \"StackOverflow_Test.csv\"\n","#predict_file_path = data_dir+predict_file_name\n"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":384,"status":"ok","timestamp":1679708630022,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"CKiryonoH4cP"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, RobertaForSequenceClassification, RobertaTokenizer\n","#          Model          | Tokenizer          | Pretrained weights shortcut\n","# MODELS = [(AutoModelForSequenceClassification,AutoTokenizer,'princeton-nlp/sup-simcse-roberta-large'),\n","#           (RobertaForSequenceClassification, RobertaTokenizer,'roberta-large'), \n","#          ]\n","\n","MODELS = {\n","    \"roberta\":{\n","        'classifier': RobertaForSequenceClassification,\n","        'tokenizer': RobertaTokenizer,\n","        'path': 'roberta-large',\n","        'logging_steps': 100,\n","        'num_train_epochs': 4,\n","        'max_length': 512,\n","        'learning_rate': 2e-5,\n","        'early_stopping_patience': 3,\n","    },\n","    \"simcse\":{\n","        'classifier': AutoModelForSequenceClassification,\n","        'tokenizer': AutoTokenizer,\n","        'path': 'princeton-nlp/sup-simcse-roberta-large',\n","        'logging_steps': 100,\n","        'num_train_epochs': 4,\n","        'max_length': 512,\n","        'learning_rate': 2e-5,\n","        'early_stopping_patience': 3000,\n","    }\n","}"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":411,"status":"ok","timestamp":1679704096144,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"LKgrqerv_b6q"},"outputs":[],"source":["\n","import numpy as np\n","\n","def read_clean_data(file_path):\n","  data = pd.read_csv(file_path)\n","  nan_value = float(\"NaN\")\n","  #Convert NaN values to empty string\n","  data[\"text\"].replace(\"\", nan_value, inplace=True)\n","  data[\"text\"].replace([np.inf, -np.inf], nan_value, inplace=True)\n","  data.dropna(subset = [\"text\"], inplace=True)\n","\n","  return data\n","\n","def get_ML_data(type, file_path = ''):\n","  # if from_file:\n","  #   if type == \"Train\":\n","  #     return read_clean_data(train_file_path)\n","\n","  #   if type == \"Validate\":\n","  #     return read_clean_data(validate_file_path)\n","\n","  #   if type == \"Test\":\n","  #     return read_clean_data(predict_file_path)\n","\n","  if len(file_path) == 0:\n","    file_path = train_file_path\n","\n","  df_ml = read_clean_data(file_path)\n","\n","  # get three dataframes, one for each set\n","  return df_ml[df_ml['MLSet'] == type]\n","  \n","      \n","def print_title(model, SEP, dataset, dataset_type):\n","  combination_title = (model+SEP+dataset+SEP+dataset_type).replace('/', '-')\n","  print('*'*80)\n","  print(combination_title.upper())\n","  print('*'*80)    \n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679704098355,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"DaRWiH0VIHbR"},"outputs":[],"source":["#https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","import torch\n","from transformers import TrainingArguments, Trainer\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, RobertaForSequenceClassification, RobertaTokenizer\n","from transformers import EarlyStoppingCallback\n","\n","# Create torch dataset\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels:\n","            item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","        \n","        # item = {key: val[idx] for key, val in self.encodings.items()}\n","        # if self.labels:\n","        #     item[\"labels\"] = self.labels[idx]\n","        # return item\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])\n","\n","\n","# ----- 2. Fine-tune pretrained model -----#\n","# Define Trainer parameters\n","def compute_metrics(p):\n","    pred, labels = p\n","    pred = np.argmax(pred, axis=1)\n","\n","    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","    recall = recall_score(y_true=labels, y_pred=pred, average='weighted')\n","    precision = precision_score(y_true=labels, y_pred=pred, average='weighted')\n","    f1 = f1_score(y_true=labels, y_pred=pred, average='weighted')\n","\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","        \n","def train_with_data(model_name, X_train, X_val, y_train, y_val, save_model=True, model_output_dir=''):\n","\n","  if len(model_output_dir)==0:\n","    model_output_dir = output_dir\n","\n","  # tokenizer_path = model_name #\"princeton-nlp/sup-simcse-roberta-large\"\n","\n","  # tokenizer = CURRENT_MODEL[1].from_pretrained(CURRENT_MODEL[2])\n","  # model = CURRENT_MODEL[0].from_pretrained(CURRENT_MODEL[2], num_labels=3)\n","\n","  global MODELS\n","\n","  CUR_MODEL = MODELS[model_name]\n","\n","  tokenizer = CUR_MODEL['tokenizer'].from_pretrained(CUR_MODEL['path'])\n","  model = CUR_MODEL['classifier'].from_pretrained(CUR_MODEL['path'], num_labels=3)\n","\n","\n","  # tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","  # model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","\n","  X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=CUR_MODEL['max_length'])\n","  X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=CUR_MODEL['max_length'])\n","\n","  train_dataset = Dataset(X_train_tokenized, y_train)\n","  val_dataset = Dataset(X_val_tokenized, y_val)\n","\n","\n","  # Define Trainer\n","  args = TrainingArguments(\n","      output_dir=model_output_dir,\n","      evaluation_strategy=\"steps\",\n","      logging_steps = CUR_MODEL['logging_steps'],\n","      per_device_train_batch_size=8,\n","      per_device_eval_batch_size=8,\n","      num_train_epochs=CUR_MODEL['num_train_epochs'],\n","      seed=0,\n","      load_best_model_at_end=True,\n","      learning_rate=CUR_MODEL['learning_rate'],\n","      \n","  )\n","  trainer = Trainer(\n","      model=model,\n","      args=args,\n","      train_dataset=train_dataset,\n","      eval_dataset=val_dataset,\n","      compute_metrics=compute_metrics,\n","      callbacks=[EarlyStoppingCallback(early_stopping_patience=CUR_MODEL['early_stopping_patience'])],\n","  )\n","\n","  # Train pre-trained model\n","  trainer.train()\n","\n","  if save_model == True:\n","    model.save_pretrained(model_output_dir)\n","  # trainer.evaluate()\n","\n","  return trainer, model\n","\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":693,"status":"ok","timestamp":1679704104383,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"wmUZsOYJG1K1"},"outputs":[],"source":["def train_with_all_data(model_name='', file_path='', model_output_dir=''):\n","  # Read data\n","  data = get_ML_data(\"Train\", file_path)\n","  X_train = list(data[\"text\"])\n","  y_train = list(data[\"label\"])\n","\n","\n","  data_val =  get_ML_data(\"Validate\", file_path)\n","  X_val = list(data_val[\"text\"])\n","  y_val = list(data_val[\"label\"])\n","\n","  # X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n","  # print(X_train)\n","\n","  if len(model_name)==0:\n","    # Define pretrained tokenizer and model\n","    # model_name = \"roberta-large\"\n","    # model_name = \"ProsusAI/finbert\"\n","    model_name = \"princeton-nlp/sup-simcse-roberta-large\"\n","\n","  train_with_data(model_name, X_train, X_val, y_train, y_val, True, model_output_dir)\n","\n","  df_val = pd.DataFrame({'text': X_val, 'label': y_val})\n","  #classify_with_data(read_clean_data(predict_file_path), \"\", \"\", True)\n","\n","#train_with_all_data()"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1679704106965,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"9b3by_uyiQOU"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","import datetime\n","\n","def write_report_internal(model, dataset_name, dataset_type, df, output_dir_report, duration, file_name, train_noise):\n","    target_names = ['positive', 'negative', 'neutral']\n","    global PREDICTION_RUN\n","    global MODELS\n","  \n","    # add model column and default value to df at the beginning of the dataframe\n","    df.insert(0, 'model', model)\n","    df.insert(1, 'dataset', dataset_name)\n","    df.insert(2, 'dataset_type', dataset_type)\n","    \n","    # get current date and time yyyy-mm-dd hh:mm\n","    now = datetime.datetime.now()\n","    dt_string = now.strftime(\"%Y-%m-%d %H:%M\")\n","    df['time'] = dt_string\n","    df['duration'] = duration\n","    df['batch'] = PREDICTION_RUN\n","    df['settings'] = str(MODELS[model])\n","    df['data_file'] = file_name\n","    df['train_noise'] = train_noise\n","    \n","\n","\n","    # append the report to the existing csv file. Append the header only once\n","    with open(output_dir_report + '/classification_report.csv', 'a') as f:\n","        df.to_csv(f, header=f.tell() == 0, index=False)\n","\n","def write_report(model, dataset_name, dataset_type, y_true, y_pred, output_dir_report, duration, file_name, train_noise):\n","    target_names = ['positive', 'negative', 'neutral']\n","\n","    # # print the report in readable format\n","    print(classification_report(y_true, y_pred, target_names=target_names))\n","    \n","    # report = classification_report(y_true, y_pred, target_names=target_names, output_dict=True)\n","    report = classification_report(y_true, y_pred, output_dict=True)\n","    df = pd.DataFrame(report).transpose()\n","    # convert the index to a column 'class'\n","    df.reset_index(level=0, inplace=True)\n","    df.rename(columns={'index': 'class'}, inplace=True)\n","    write_report_internal(model, dataset_name, dataset_type, df, output_dir_report, duration, file_name, train_noise)\n","\n","    # acc, pre, rec, f1 = compute_performance_opiner(y_true, y_pred)\n","    # df_opiner = pd.DataFrame({'class':['opiner'], 'support': [acc], 'precision': [pre], 'recall': [rec], 'f1': [f1]})\n","    # df_opiner.reset_index(level=0, inplace=True)\n","    # df.rename(columns={'index': 'class'}, inplace=True)\n","    # # df_opiner['class']='opiner'\n","    # write_report_internal(model, dataset_name, dataset_type, df_opiner, output_dir_report, duration)\n","\n","\n","def additional_classification(dataset_type, additional_testing_file_name, model, SEP, dataset, output_dir, duration, file_name, train_noise):\n","  combination_title = (model+SEP+dataset+SEP+dataset_type+SEP+train_noise).replace('/', '-')\n","  print_title(model, SEP, dataset, dataset_type)\n","\n","  input_file_path = os.path.join(dataset_dir, additional_testing_file_name)\n","  prediction_output_file = os.path.join(prediction_output_dir, combination_title+SEP+additional_testing_file_name)\n","  accuracy, y_true, y_pred = classify(input_file_path, prediction_output_file, True, model_output_dir)\n","  write_report(model, dataset, dataset_type, y_true, y_pred, output_dir, duration, file_name, train_noise)\n","\n","    "]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":1351,"status":"ok","timestamp":1679704111304,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"ed4Z2pVD12Sv"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, RobertaForSequenceClassification, RobertaTokenizer\n","from transformers import pipeline\n","from transformers.pipelines.pt_utils import KeyDataset\n","from transformers import TrainingArguments, Trainer\n","import torch\n","\n","import numpy as np\n","import pandas as pd\n","\n","# Create torch dataset\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels:\n","            item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])\n","\n","  \n","def classify_with_data(test_data, output_file_path, generate_report, model_path ='', trainer = None):\n","  \n","  X_test = list(test_data[\"text\"])\n","  \n","  # tokenizer_path = \"princeton-nlp/sup-simcse-roberta-large\"\n","  # tokenizer_path = 'roberta-large'\n","  #tokenizer_path = \"ProsusAI/finbert\"\n","  #tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n","  \n","  # Since SimCSE also uses Roberta model, we are loading from 'roberta-large'.\n","  if model_path == 'cardiffnlp/twitter-roberta-base-sentiment-latest':\n","    print(\"*-\"*100)\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","  else:\n","    tokenizer = AutoTokenizer.from_pretrained('roberta-large')\n","\n","  max_length=512\n","\n","  X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=max_length)\n","  test_dataset = Dataset(X_test_tokenized)\n","\n","  \n","  # # # Define test trainer\n","  if trainer != None:\n","    test_trainer = trainer\n","  else:\n","    if len(model_path) == 0:\n","      #model_path = \"ProsusAI/finbert\"\n","      #model_path = tokenizer_path\n","      model_path = output_dir\n","\n","    print(\"MODEL PATH --------- \"+model_path)\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(model_path) \n","\n","    test_trainer = Trainer(model)\n","\n","  # # Make prediction\n","  raw_pred, _, _ = test_trainer.predict(test_dataset)\n","  \n","  # check if the model path is cardiffnlp/twitter-roberta-base-sentiment-latest\n","  if model_path == 'cardiffnlp/twitter-roberta-base-sentiment-latest':\n","        # move all the values in 3rd column to 1st column, and the values in the 1s column to 2nd column, 2nd column to 3rd column\n","    # since cardiffnlp/twitter-roberta-base-sentiment-latest provides output in neg/post/neutral order.\n","    raw_pred = np.roll(raw_pred, 1, axis=1)\n","\n","  # Preprocess raw predictions\n","  data = test_data\n","  y_pred = np.argmax(raw_pred, axis=1)\n","  model_name = model_path.split('/')[-1] \n","  data['sentiment-ml'+\"-\"+model_name] = y_pred\n","  prob = np.array(raw_pred)\n","  data['positive'+\"-\"+model_name] = prob[:,0]\n","  data['negative'+\"-\"+model_name] = prob[:,1]\n","  data['neutral'+\"-\"+model_name] = prob[:,2]\n","  # data['model'] = model_path.split('/')[-1] \n","\n","\n","  if len(output_file_path) > 0:\n","    # file_path = output_file_path.replace(\".csv\", \"_\"+DATE+ str(PREDICTION_RUN) +\".csv\")\n","    \n","    # THIS IS TEMPORARY FOR CHECKING MULTIPLE MODEL PERFORMANCE\n","    file_path = output_file_path\n","    data.to_csv(file_path, index=False)\n","\n","    print(\"Written prediction results to file: \"+ file_path)\n","\n","  if generate_report == True: #validation case only\n","    y_test = list(test_data[\"label\"])\n","    from sklearn.metrics import classification_report\n","    print(classification_report(y_test, y_pred.tolist()))\n","    f1score = f1_score(y_test, y_pred, average='weighted')\n","    print(f1score)\n","    return f1score, y_test, y_pred.tolist()\n","    \n","\n","  # ----- 3. Classify/Predict -----#\n","def classify(input_file_path, output_file_path, generate_report, model_path=''):\n","  test_data = get_ML_data(\"Test\", input_file_path)\n","  file_path = output_file_path.replace(\".csv\", \"_\"+DATE+ str(PREDICTION_RUN) +\"_relevant.csv\")\n","\n","\n","  return classify_with_data(test_data, output_file_path, generate_report, model_path)\n","\n","\n","#PREDICTION_RUN += 1"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4789592,"status":"ok","timestamp":1679713431752,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"gn8Vovm9X5wy","outputId":"cf0a759c-f35a-4c17-8a57-d4fe7e2af7b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["********************************************************************************\n","SIMCSE--4.3.BINLIN-SO--ML\n","********************************************************************************\n","/content/machine-learning/model-preparation/0.Combined-Dataset/65.4.3.StackOverflow-data-ML-auto-emotion-double-Train.csv\n","/content/machine-learning/model-output/model_files/simcse--4.3.BinLin-SO--ML--Auto-Emotion-D-T-All\n","/content/machine-learning/model-output/prediction_files/simcse--4.3.BinLin-SO--ML--Auto-Emotion-D-T-All--65.4.3.StackOverflow-data-ML-auto-emotion-double-Train.csv\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-large were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1052' max='1052' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1052/1052 03:09, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.564800</td>\n","      <td>0.521031</td>\n","      <td>0.827243</td>\n","      <td>0.753090</td>\n","      <td>0.827243</td>\n","      <td>0.784275</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.388500</td>\n","      <td>0.376401</td>\n","      <td>0.893688</td>\n","      <td>0.887934</td>\n","      <td>0.893688</td>\n","      <td>0.888676</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.259300</td>\n","      <td>0.626111</td>\n","      <td>0.877076</td>\n","      <td>0.871704</td>\n","      <td>0.877076</td>\n","      <td>0.873533</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.152800</td>\n","      <td>0.724849</td>\n","      <td>0.887043</td>\n","      <td>0.893853</td>\n","      <td>0.887043</td>\n","      <td>0.889679</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.156900</td>\n","      <td>0.649864</td>\n","      <td>0.890365</td>\n","      <td>0.890022</td>\n","      <td>0.890365</td>\n","      <td>0.890096</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.093900</td>\n","      <td>0.792493</td>\n","      <td>0.877076</td>\n","      <td>0.871405</td>\n","      <td>0.877076</td>\n","      <td>0.873096</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.023200</td>\n","      <td>0.732964</td>\n","      <td>0.893688</td>\n","      <td>0.891841</td>\n","      <td>0.893688</td>\n","      <td>0.892522</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.000600</td>\n","      <td>0.902329</td>\n","      <td>0.880399</td>\n","      <td>0.876847</td>\n","      <td>0.880399</td>\n","      <td>0.878123</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.006400</td>\n","      <td>0.924186</td>\n","      <td>0.880399</td>\n","      <td>0.876658</td>\n","      <td>0.880399</td>\n","      <td>0.878039</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.003400</td>\n","      <td>0.920187</td>\n","      <td>0.883721</td>\n","      <td>0.880486</td>\n","      <td>0.883721</td>\n","      <td>0.881740</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["********************************************************************************\n","SIMCSE--4.3.BINLIN-SO--ML\n","********************************************************************************\n","MODEL PATH --------- /content/machine-learning/model-output/model_files/simcse--4.3.BinLin-SO--ML--Auto-Emotion-D-T-All\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Written prediction results to file: /content/machine-learning/model-output/prediction_files/simcse--4.3.BinLin-SO--ML--Auto-Emotion-D-T-All--65.4.3.StackOverflow-data-ML-auto-emotion-double-Train.csv\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.69      0.69        26\n","           1       0.72      0.72      0.72        36\n","           2       0.92      0.92      0.92       238\n","\n","    accuracy                           0.88       300\n","   macro avg       0.78      0.78      0.78       300\n","weighted avg       0.88      0.88      0.88       300\n","\n","0.88\n","              precision    recall  f1-score   support\n","\n","    positive       0.69      0.69      0.69        26\n","    negative       0.72      0.72      0.72        36\n","     neutral       0.92      0.92      0.92       238\n","\n","    accuracy                           0.88       300\n","   macro avg       0.78      0.78      0.78       300\n","weighted avg       0.88      0.88      0.88       300\n","\n","********************************************************************************\n","SIMCSE--5.SENTI4SD-GOLDSTANDARD-EMOTIONPOLARITY--ML\n","********************************************************************************\n","/content/machine-learning/model-preparation/0.Combined-Dataset/66.05.Senti4SD_GoldStandard_EmotionPolarity-ML-auto-emotion-double-Train.csv\n","/content/machine-learning/model-output/model_files/simcse--5.Senti4SD-GoldStandard-EmotionPolarity--ML--Auto-Emotion-D-T-All\n","/content/machine-learning/model-output/prediction_files/simcse--5.Senti4SD-GoldStandard-EmotionPolarity--ML--Auto-Emotion-D-T-All--66.05.Senti4SD_GoldStandard_EmotionPolarity-ML-auto-emotion-double-Train.csv\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-large were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3096' max='3096' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3096/3096 26:34, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.617800</td>\n","      <td>0.739466</td>\n","      <td>0.772881</td>\n","      <td>0.809838</td>\n","      <td>0.772881</td>\n","      <td>0.774970</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.494000</td>\n","      <td>0.602363</td>\n","      <td>0.846328</td>\n","      <td>0.857428</td>\n","      <td>0.846328</td>\n","      <td>0.845147</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.433800</td>\n","      <td>0.651905</td>\n","      <td>0.816949</td>\n","      <td>0.828353</td>\n","      <td>0.816949</td>\n","      <td>0.817778</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.344200</td>\n","      <td>0.568850</td>\n","      <td>0.822599</td>\n","      <td>0.837948</td>\n","      <td>0.822599</td>\n","      <td>0.821707</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.459900</td>\n","      <td>0.919604</td>\n","      <td>0.789831</td>\n","      <td>0.829041</td>\n","      <td>0.789831</td>\n","      <td>0.791040</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.395300</td>\n","      <td>0.538020</td>\n","      <td>0.827119</td>\n","      <td>0.847939</td>\n","      <td>0.827119</td>\n","      <td>0.829112</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.353100</td>\n","      <td>0.806747</td>\n","      <td>0.822599</td>\n","      <td>0.833796</td>\n","      <td>0.822599</td>\n","      <td>0.822966</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.215100</td>\n","      <td>1.497042</td>\n","      <td>0.763842</td>\n","      <td>0.821168</td>\n","      <td>0.763842</td>\n","      <td>0.764077</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.180200</td>\n","      <td>1.795729</td>\n","      <td>0.729944</td>\n","      <td>0.793704</td>\n","      <td>0.729944</td>\n","      <td>0.729575</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.164800</td>\n","      <td>1.020162</td>\n","      <td>0.820339</td>\n","      <td>0.831531</td>\n","      <td>0.820339</td>\n","      <td>0.821815</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.175000</td>\n","      <td>1.501788</td>\n","      <td>0.793220</td>\n","      <td>0.822103</td>\n","      <td>0.793220</td>\n","      <td>0.794683</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.223700</td>\n","      <td>1.184367</td>\n","      <td>0.801130</td>\n","      <td>0.818700</td>\n","      <td>0.801130</td>\n","      <td>0.802867</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.159800</td>\n","      <td>1.523105</td>\n","      <td>0.774011</td>\n","      <td>0.815305</td>\n","      <td>0.774011</td>\n","      <td>0.775963</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.154800</td>\n","      <td>1.913353</td>\n","      <td>0.744633</td>\n","      <td>0.797887</td>\n","      <td>0.744633</td>\n","      <td>0.745491</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.197300</td>\n","      <td>1.321314</td>\n","      <td>0.794350</td>\n","      <td>0.828157</td>\n","      <td>0.794350</td>\n","      <td>0.795381</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.089500</td>\n","      <td>1.388225</td>\n","      <td>0.797740</td>\n","      <td>0.823076</td>\n","      <td>0.797740</td>\n","      <td>0.798839</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.062400</td>\n","      <td>1.303897</td>\n","      <td>0.823729</td>\n","      <td>0.836329</td>\n","      <td>0.823729</td>\n","      <td>0.824521</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.093200</td>\n","      <td>1.472684</td>\n","      <td>0.802260</td>\n","      <td>0.826897</td>\n","      <td>0.802260</td>\n","      <td>0.803042</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.020700</td>\n","      <td>1.688800</td>\n","      <td>0.787571</td>\n","      <td>0.821204</td>\n","      <td>0.787571</td>\n","      <td>0.789192</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.036500</td>\n","      <td>1.749909</td>\n","      <td>0.787571</td>\n","      <td>0.811573</td>\n","      <td>0.787571</td>\n","      <td>0.789135</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.052100</td>\n","      <td>1.909596</td>\n","      <td>0.766102</td>\n","      <td>0.806139</td>\n","      <td>0.766102</td>\n","      <td>0.767373</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.031200</td>\n","      <td>1.576442</td>\n","      <td>0.796610</td>\n","      <td>0.812227</td>\n","      <td>0.796610</td>\n","      <td>0.798080</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.028500</td>\n","      <td>1.906574</td>\n","      <td>0.774011</td>\n","      <td>0.812379</td>\n","      <td>0.774011</td>\n","      <td>0.775597</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.022300</td>\n","      <td>2.004519</td>\n","      <td>0.768362</td>\n","      <td>0.804987</td>\n","      <td>0.768362</td>\n","      <td>0.769481</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.007700</td>\n","      <td>1.962153</td>\n","      <td>0.772881</td>\n","      <td>0.799973</td>\n","      <td>0.772881</td>\n","      <td>0.774475</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.010000</td>\n","      <td>2.022206</td>\n","      <td>0.771751</td>\n","      <td>0.801733</td>\n","      <td>0.771751</td>\n","      <td>0.773222</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.036700</td>\n","      <td>2.014153</td>\n","      <td>0.772881</td>\n","      <td>0.805516</td>\n","      <td>0.772881</td>\n","      <td>0.774499</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.025400</td>\n","      <td>1.863164</td>\n","      <td>0.779661</td>\n","      <td>0.810290</td>\n","      <td>0.779661</td>\n","      <td>0.781298</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.009000</td>\n","      <td>1.780652</td>\n","      <td>0.787571</td>\n","      <td>0.813674</td>\n","      <td>0.787571</td>\n","      <td>0.788850</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.008600</td>\n","      <td>1.811624</td>\n","      <td>0.784181</td>\n","      <td>0.811589</td>\n","      <td>0.784181</td>\n","      <td>0.785531</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["********************************************************************************\n","SIMCSE--5.SENTI4SD-GOLDSTANDARD-EMOTIONPOLARITY--ML\n","********************************************************************************\n","MODEL PATH --------- /content/machine-learning/model-output/model_files/simcse--5.Senti4SD-GoldStandard-EmotionPolarity--ML--Auto-Emotion-D-T-All\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Written prediction results to file: /content/machine-learning/model-output/prediction_files/simcse--5.Senti4SD-GoldStandard-EmotionPolarity--ML--Auto-Emotion-D-T-All--66.05.Senti4SD_GoldStandard_EmotionPolarity-ML-auto-emotion-double-Train.csv\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.85      0.91       306\n","           1       0.88      0.62      0.73       240\n","           2       0.70      0.94      0.81       340\n","\n","    accuracy                           0.82       886\n","   macro avg       0.86      0.80      0.81       886\n","weighted avg       0.85      0.82      0.82       886\n","\n","0.8209153018203094\n","              precision    recall  f1-score   support\n","\n","    positive       0.98      0.85      0.91       306\n","    negative       0.88      0.62      0.73       240\n","     neutral       0.70      0.94      0.81       340\n","\n","    accuracy                           0.82       886\n","   macro avg       0.86      0.80      0.81       886\n","weighted avg       0.85      0.82      0.82       886\n","\n","********************************************************************************\n","SIMCSE--6.OPINER-STACKOVERFLOW--ML\n","********************************************************************************\n","/content/machine-learning/model-preparation/0.Combined-Dataset/67.6.Opiner-ML-auto-emotion-double-Train.csv\n","/content/machine-learning/model-output/model_files/simcse--6.Opiner-StackOverflow--ML--Auto-Emotion-D-T-All\n","/content/machine-learning/model-output/prediction_files/simcse--6.Opiner-StackOverflow--ML--Auto-Emotion-D-T-All--67.6.Opiner-ML-auto-emotion-double-Train.csv\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-large were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3164' max='3164' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3164/3164 37:19, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.936200</td>\n","      <td>0.891431</td>\n","      <td>0.598895</td>\n","      <td>0.682673</td>\n","      <td>0.598895</td>\n","      <td>0.496814</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.853500</td>\n","      <td>0.915575</td>\n","      <td>0.626519</td>\n","      <td>0.606789</td>\n","      <td>0.626519</td>\n","      <td>0.579928</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.861800</td>\n","      <td>0.907345</td>\n","      <td>0.608840</td>\n","      <td>0.588538</td>\n","      <td>0.608840</td>\n","      <td>0.575072</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.771900</td>\n","      <td>0.866482</td>\n","      <td>0.617680</td>\n","      <td>0.598371</td>\n","      <td>0.617680</td>\n","      <td>0.583147</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.767300</td>\n","      <td>0.863381</td>\n","      <td>0.664088</td>\n","      <td>0.664384</td>\n","      <td>0.664088</td>\n","      <td>0.664094</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.766700</td>\n","      <td>0.855550</td>\n","      <td>0.635359</td>\n","      <td>0.621017</td>\n","      <td>0.635359</td>\n","      <td>0.606968</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.741900</td>\n","      <td>0.881492</td>\n","      <td>0.645304</td>\n","      <td>0.629379</td>\n","      <td>0.645304</td>\n","      <td>0.632017</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.714300</td>\n","      <td>0.863990</td>\n","      <td>0.650829</td>\n","      <td>0.633932</td>\n","      <td>0.650829</td>\n","      <td>0.613220</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.539600</td>\n","      <td>1.019452</td>\n","      <td>0.623204</td>\n","      <td>0.603132</td>\n","      <td>0.623204</td>\n","      <td>0.589785</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.518900</td>\n","      <td>1.237181</td>\n","      <td>0.613260</td>\n","      <td>0.615149</td>\n","      <td>0.613260</td>\n","      <td>0.578307</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.539900</td>\n","      <td>1.006163</td>\n","      <td>0.656354</td>\n","      <td>0.644062</td>\n","      <td>0.656354</td>\n","      <td>0.647113</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.485800</td>\n","      <td>1.113913</td>\n","      <td>0.649724</td>\n","      <td>0.635728</td>\n","      <td>0.649724</td>\n","      <td>0.638901</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.473100</td>\n","      <td>1.076593</td>\n","      <td>0.639779</td>\n","      <td>0.627686</td>\n","      <td>0.639779</td>\n","      <td>0.631437</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.478100</td>\n","      <td>1.275323</td>\n","      <td>0.638674</td>\n","      <td>0.648343</td>\n","      <td>0.638674</td>\n","      <td>0.642688</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.415500</td>\n","      <td>1.506037</td>\n","      <td>0.637569</td>\n","      <td>0.624001</td>\n","      <td>0.637569</td>\n","      <td>0.604845</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.355200</td>\n","      <td>1.317896</td>\n","      <td>0.641989</td>\n","      <td>0.640940</td>\n","      <td>0.641989</td>\n","      <td>0.641371</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.270300</td>\n","      <td>1.547313</td>\n","      <td>0.595580</td>\n","      <td>0.567778</td>\n","      <td>0.595580</td>\n","      <td>0.570832</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.180200</td>\n","      <td>1.996173</td>\n","      <td>0.627624</td>\n","      <td>0.620773</td>\n","      <td>0.627624</td>\n","      <td>0.622367</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.238300</td>\n","      <td>1.939823</td>\n","      <td>0.600000</td>\n","      <td>0.616055</td>\n","      <td>0.600000</td>\n","      <td>0.604399</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.267700</td>\n","      <td>1.880248</td>\n","      <td>0.593370</td>\n","      <td>0.588871</td>\n","      <td>0.593370</td>\n","      <td>0.590695</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.193600</td>\n","      <td>2.199404</td>\n","      <td>0.615470</td>\n","      <td>0.603986</td>\n","      <td>0.615470</td>\n","      <td>0.605526</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.194700</td>\n","      <td>2.243052</td>\n","      <td>0.614365</td>\n","      <td>0.598000</td>\n","      <td>0.614365</td>\n","      <td>0.601151</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.186000</td>\n","      <td>2.318166</td>\n","      <td>0.614365</td>\n","      <td>0.604968</td>\n","      <td>0.614365</td>\n","      <td>0.608362</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.199200</td>\n","      <td>2.231989</td>\n","      <td>0.612155</td>\n","      <td>0.596330</td>\n","      <td>0.612155</td>\n","      <td>0.600839</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.116000</td>\n","      <td>2.271206</td>\n","      <td>0.615470</td>\n","      <td>0.614689</td>\n","      <td>0.615470</td>\n","      <td>0.614776</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.110000</td>\n","      <td>2.458370</td>\n","      <td>0.617680</td>\n","      <td>0.603230</td>\n","      <td>0.617680</td>\n","      <td>0.607242</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.119700</td>\n","      <td>2.505412</td>\n","      <td>0.611050</td>\n","      <td>0.602631</td>\n","      <td>0.611050</td>\n","      <td>0.605018</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.062300</td>\n","      <td>2.539707</td>\n","      <td>0.625414</td>\n","      <td>0.611118</td>\n","      <td>0.625414</td>\n","      <td>0.613953</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.075500</td>\n","      <td>2.560835</td>\n","      <td>0.620994</td>\n","      <td>0.607590</td>\n","      <td>0.620994</td>\n","      <td>0.611080</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.078000</td>\n","      <td>2.552556</td>\n","      <td>0.614365</td>\n","      <td>0.603308</td>\n","      <td>0.614365</td>\n","      <td>0.607411</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.099600</td>\n","      <td>2.562931</td>\n","      <td>0.618785</td>\n","      <td>0.607100</td>\n","      <td>0.618785</td>\n","      <td>0.610955</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["********************************************************************************\n","SIMCSE--6.OPINER-STACKOVERFLOW--ML\n","********************************************************************************\n","MODEL PATH --------- /content/machine-learning/model-output/model_files/simcse--6.Opiner-StackOverflow--ML--Auto-Emotion-D-T-All\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Written prediction results to file: /content/machine-learning/model-output/prediction_files/simcse--6.Opiner-StackOverflow--ML--Auto-Emotion-D-T-All--67.6.Opiner-ML-auto-emotion-double-Train.csv\n","              precision    recall  f1-score   support\n","\n","           0       0.51      0.56      0.54       210\n","           1       0.63      0.54      0.58       168\n","           2       0.77      0.78      0.78       528\n","\n","    accuracy                           0.68       906\n","   macro avg       0.64      0.63      0.63       906\n","weighted avg       0.69      0.68      0.68       906\n","\n","0.684168948253111\n","              precision    recall  f1-score   support\n","\n","    positive       0.51      0.56      0.54       210\n","    negative       0.63      0.54      0.58       168\n","     neutral       0.77      0.78      0.78       528\n","\n","    accuracy                           0.68       906\n","   macro avg       0.64      0.63      0.63       906\n","weighted avg       0.69      0.68      0.68       906\n","\n","********************************************************************************\n","SIMCSE--7.1.BERT4SENTISE-STACKOVERFLOW--ML\n","********************************************************************************\n","/content/machine-learning/model-preparation/0.Combined-Dataset/68.7.1.BERT4SentiSE.SO-ML-auto-emotion-double-Train.csv\n","/content/machine-learning/model-output/model_files/simcse--7.1.BERT4SentiSE-StackOverflow--ML--Auto-Emotion-D-T-All\n","/content/machine-learning/model-output/prediction_files/simcse--7.1.BERT4SentiSE-StackOverflow--ML--Auto-Emotion-D-T-All--68.7.1.BERT4SentiSE.SO-ML-auto-emotion-double-Train.csv\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-large were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2800' max='2800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2800/2800 11:20, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.522700</td>\n","      <td>0.363481</td>\n","      <td>0.877653</td>\n","      <td>0.879435</td>\n","      <td>0.877653</td>\n","      <td>0.877655</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.408100</td>\n","      <td>0.343885</td>\n","      <td>0.902622</td>\n","      <td>0.904501</td>\n","      <td>0.902622</td>\n","      <td>0.903047</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.300500</td>\n","      <td>0.513331</td>\n","      <td>0.892634</td>\n","      <td>0.895950</td>\n","      <td>0.892634</td>\n","      <td>0.893020</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.302900</td>\n","      <td>0.562039</td>\n","      <td>0.881398</td>\n","      <td>0.883542</td>\n","      <td>0.881398</td>\n","      <td>0.878129</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.295500</td>\n","      <td>0.528109</td>\n","      <td>0.880150</td>\n","      <td>0.882660</td>\n","      <td>0.880150</td>\n","      <td>0.875512</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.275300</td>\n","      <td>0.469686</td>\n","      <td>0.901373</td>\n","      <td>0.911732</td>\n","      <td>0.901373</td>\n","      <td>0.903536</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.264900</td>\n","      <td>0.477591</td>\n","      <td>0.902622</td>\n","      <td>0.902353</td>\n","      <td>0.902622</td>\n","      <td>0.901814</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.082800</td>\n","      <td>0.924314</td>\n","      <td>0.861423</td>\n","      <td>0.870240</td>\n","      <td>0.861423</td>\n","      <td>0.855684</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.124600</td>\n","      <td>0.529661</td>\n","      <td>0.923845</td>\n","      <td>0.924199</td>\n","      <td>0.923845</td>\n","      <td>0.923811</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.091100</td>\n","      <td>0.578633</td>\n","      <td>0.906367</td>\n","      <td>0.905985</td>\n","      <td>0.906367</td>\n","      <td>0.905577</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.099800</td>\n","      <td>0.621830</td>\n","      <td>0.897628</td>\n","      <td>0.897491</td>\n","      <td>0.897628</td>\n","      <td>0.897168</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.061300</td>\n","      <td>0.676740</td>\n","      <td>0.903870</td>\n","      <td>0.908027</td>\n","      <td>0.903870</td>\n","      <td>0.904751</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.077300</td>\n","      <td>0.578181</td>\n","      <td>0.911361</td>\n","      <td>0.911748</td>\n","      <td>0.911361</td>\n","      <td>0.911219</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.024600</td>\n","      <td>0.678708</td>\n","      <td>0.908864</td>\n","      <td>0.908902</td>\n","      <td>0.908864</td>\n","      <td>0.908673</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.018300</td>\n","      <td>0.729783</td>\n","      <td>0.901373</td>\n","      <td>0.900379</td>\n","      <td>0.901373</td>\n","      <td>0.900313</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.008300</td>\n","      <td>0.849508</td>\n","      <td>0.888889</td>\n","      <td>0.890660</td>\n","      <td>0.888889</td>\n","      <td>0.887660</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.031400</td>\n","      <td>0.772679</td>\n","      <td>0.897628</td>\n","      <td>0.899112</td>\n","      <td>0.897628</td>\n","      <td>0.897169</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.030700</td>\n","      <td>0.777004</td>\n","      <td>0.903870</td>\n","      <td>0.903277</td>\n","      <td>0.903870</td>\n","      <td>0.902989</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.030100</td>\n","      <td>0.729973</td>\n","      <td>0.906367</td>\n","      <td>0.905732</td>\n","      <td>0.906367</td>\n","      <td>0.905910</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.017400</td>\n","      <td>0.750709</td>\n","      <td>0.903870</td>\n","      <td>0.902759</td>\n","      <td>0.903870</td>\n","      <td>0.902832</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.005300</td>\n","      <td>0.729638</td>\n","      <td>0.913858</td>\n","      <td>0.913648</td>\n","      <td>0.913858</td>\n","      <td>0.913738</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.000100</td>\n","      <td>0.756588</td>\n","      <td>0.910112</td>\n","      <td>0.909644</td>\n","      <td>0.910112</td>\n","      <td>0.909732</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.000100</td>\n","      <td>0.769253</td>\n","      <td>0.911361</td>\n","      <td>0.910971</td>\n","      <td>0.911361</td>\n","      <td>0.911056</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.009300</td>\n","      <td>0.796622</td>\n","      <td>0.903870</td>\n","      <td>0.903605</td>\n","      <td>0.903870</td>\n","      <td>0.903347</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.003100</td>\n","      <td>0.817340</td>\n","      <td>0.902622</td>\n","      <td>0.902475</td>\n","      <td>0.902622</td>\n","      <td>0.902222</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.000100</td>\n","      <td>0.780967</td>\n","      <td>0.908864</td>\n","      <td>0.908586</td>\n","      <td>0.908864</td>\n","      <td>0.908627</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.000100</td>\n","      <td>0.774632</td>\n","      <td>0.911361</td>\n","      <td>0.911190</td>\n","      <td>0.911361</td>\n","      <td>0.911251</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.000100</td>\n","      <td>0.774757</td>\n","      <td>0.910112</td>\n","      <td>0.910022</td>\n","      <td>0.910112</td>\n","      <td>0.910054</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["********************************************************************************\n","SIMCSE--7.1.BERT4SENTISE-STACKOVERFLOW--ML\n","********************************************************************************\n","MODEL PATH --------- /content/machine-learning/model-output/model_files/simcse--7.1.BERT4SentiSE-StackOverflow--ML--Auto-Emotion-D-T-All\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Written prediction results to file: /content/machine-learning/model-output/prediction_files/simcse--7.1.BERT4SentiSE-StackOverflow--ML--Auto-Emotion-D-T-All--68.7.1.BERT4SentiSE.SO-ML-auto-emotion-double-Train.csv\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.76      0.84       116\n","           1       0.97      0.80      0.88       224\n","           2       0.86      0.97      0.91       460\n","\n","    accuracy                           0.90       800\n","   macro avg       0.92      0.85      0.88       800\n","weighted avg       0.90      0.90      0.89       800\n","\n","0.8930917537746806\n","              precision    recall  f1-score   support\n","\n","    positive       0.94      0.76      0.84       116\n","    negative       0.97      0.80      0.88       224\n","     neutral       0.86      0.97      0.91       460\n","\n","    accuracy                           0.90       800\n","   macro avg       0.92      0.85      0.88       800\n","weighted avg       0.90      0.90      0.89       800\n","\n"]}],"source":["import os\n","import time\n","\n","# read data_files.csv file and go through each file and perform cross validation\n","df_data_files = pd.read_csv(os.path.join(dataset_dir, 'data_files.csv'))\n","\n","# models = [\"roberta-large\"]\n","# models = [\"princeton-nlp/sup-simcse-roberta-large\"]\n","# models = [\"roberta-large\", \"princeton-nlp/sup-simcse-roberta-large\"]\n","# models = [\"roberta\", \"simcse\"]\n","# models = [\"roberta\"]\n","models = [\"simcse\"]\n","combine_classification_report = os.path.join(output_dir, \"classification_report.csv\")\n","\n","# loop through each row in the dataframe\n","for index, row in df_data_files.iterrows():\n","  for model in models:\n","      # get the file path\n","      dataset = row['dataset']\n","      dataset_type = row['type']\n","      file_name = row['file']\n","      train = (row['train'] == 'yes')\n","      train_noise = row['train_noise']\n","      if train == False:\n","        continue\n","\n","      SEP = '--'\n","      combination_title = (model+SEP+dataset+SEP+dataset_type+SEP+train_noise).replace('/', '-')\n","      print_title(model, SEP, dataset, dataset_type)\n","\n","      input_file_path = os.path.join(dataset_dir, file_name)\n","      model_output_dir = os.path.join(model_output_dir_base, combination_title)\n","      prediction_output_file = os.path.join(prediction_output_dir, combination_title+SEP+file_name)\n","\n","\n","      print(input_file_path)\n","      print(model_output_dir)\n","      print(prediction_output_file)\n","\n","      if not os.path.exists(model_output_dir):\n","        os.makedirs(model_output_dir)\n","      #PREDICTION_RUN += 1\n","  \n","      start_time = time.time()\n","\n","      train_with_all_data(model, input_file_path, model_output_dir)\n","      # continue \n","\n","      end_time = time.time()\n","      duration = end_time - start_time\n","\n","      print_title(model, SEP, dataset, dataset_type)\n","      f1score, y_true, y_pred = classify(input_file_path, prediction_output_file, True, model_output_dir)\n","\n","      write_report(model, dataset, dataset_type, y_true, y_pred, output_dir, duration, file_name, train_noise)\n","\n","      # additional_testing_file_name    = 'library-selection-factors.csv'\n","      # run_additional_testing = True\n","\n","      if run_additional_testing:\n","        dataset_type = 'Factors'\n","        additional_testing_file_name    = 'library-selection-factors.csv'\n","        additional_classification(dataset_type, additional_testing_file_name, model, SEP, dataset, output_dir, duration, file_name, train_noise)\n","\n","        dataset_type = 'Factors-Fixed-emotion'\n","        additional_testing_file_name    = 'library-selection-factors-emotion.csv'\n","        additional_classification(dataset_type, additional_testing_file_name, model, SEP, dataset, output_dir, duration, file_name, train_noise)\n","        \n","        dataset_type = 'Factors-Auto-emotion'\n","        additional_testing_file_name    = 'library-selection-factors-auto-emotion.csv'\n","        additional_classification(dataset_type, additional_testing_file_name, model, SEP, dataset, output_dir, duration, file_name, train_noise)\n"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1679713431753,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"1Hmc26q_VPJq"},"outputs":[],"source":["#cp /content/machine-learning/model-output/classification_report.csv /content/drive/MyDrive//machine-learning-output/\n","!cp /content/machine-learning/model-output/classification_report.csv /content/drive/MyDrive//machine-learning/model-output"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1679713432389,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"ZRqGMFx7zcQK","outputId":"38ac035b-c20e-45c2-b615-20a287cf76ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["sending incremental file list\n","prediction_files/simcse--4.3.BinLin-SO--ML--Auto-Emotion-D-T-All--65.4.3.StackOverflow-data-ML-auto-emotion-double-Train.csv\n","\r         32,768  45%    0.00kB/s    0:00:00  \r         72,478 100%   37.87MB/s    0:00:00 (xfr#1, to-chk=35/83)\n","prediction_files/simcse--5.Senti4SD-GoldStandard-EmotionPolarity--ML--Auto-Emotion-D-T-All--66.05.Senti4SD_GoldStandard_EmotionPolarity-ML-auto-emotion-double-Train.csv\n","\r         32,768   7%    3.12MB/s    0:00:00  \r        454,545 100%   17.34MB/s    0:00:00 (xfr#2, to-chk=25/83)\n","prediction_files/simcse--6.Opiner-StackOverflow--ML--Auto-Emotion-D-T-All--67.6.Opiner-ML-auto-emotion-double-Train.csv\n","\r         32,768  10%    1.25MB/s    0:00:00  \r        300,753 100%    6.37MB/s    0:00:00 (xfr#3, to-chk=17/83)\n","prediction_files/simcse--7.1.BERT4SentiSE-StackOverflow--ML--Auto-Emotion-D-T-All--68.7.1.BERT4SentiSE.SO-ML-auto-emotion-double-Train.csv\n","\r         32,768  15%  711.11kB/s    0:00:00  \r        211,397 100%    3.15MB/s    0:00:00 (xfr#4, to-chk=6/83)\n","\n","sent 1,047,895 bytes  received 105 bytes  2,096,000.00 bytes/sec\n","total size is 28,255,487  speedup is 26.96\n"]}],"source":["#!cp -r               /content/machine-learning/model-output/* /content/drive/MyDrive//machine-learning/model-output/\n","!rsync -av --progress /content/machine-learning/model-output/* /content/drive/MyDrive//machine-learning/model-output/ --exclude model_files/"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"elapsed":950247,"status":"ok","timestamp":1679698659949,"user":{"displayName":"Minaoar Hossain Tanzil","userId":"12260630854133459662"},"user_tz":360},"id":"B3eJcg8OtCA6","outputId":"f3cff140-f427-44a3-bd1c-aa619d33b40b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv\n","/content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv\n","/content/machine-learning/model-output/model_files/simcse--4.3.BinLin-SO--ML--Auto-Emotion-D-T\n","MODEL PATH --------- /content/machine-learning/model-output/model_files/simcse--4.3.BinLin-SO--ML--Auto-Emotion-D-T\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Written prediction results to file: /content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv\n","/content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv\n","/content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv\n","/content/machine-learning/model-output/model_files/simcse--5.Senti4SD-GoldStandard-EmotionPolarity--ML--Auto-Emotion-D-T\n","MODEL PATH --------- /content/machine-learning/model-output/model_files/simcse--5.Senti4SD-GoldStandard-EmotionPolarity--ML--Auto-Emotion-D-T\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Written prediction results to file: /content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv\n","/content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv\n","/content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv\n","/content/machine-learning/model-output/model_files/simcse--6.Opiner-StackOverflow--ML--Auto-Emotion-D-T\n","MODEL PATH --------- /content/machine-learning/model-output/model_files/simcse--6.Opiner-StackOverflow--ML--Auto-Emotion-D-T\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Written prediction results to file: /content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv\n","/content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv\n","/content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv\n","/content/machine-learning/model-output/model_files/simcse--7.1.BERT4SentiSE-StackOverflow--ML--Auto-Emotion-D-T\n","MODEL PATH --------- /content/machine-learning/model-output/model_files/simcse--7.1.BERT4SentiSE-StackOverflow--ML--Auto-Emotion-D-T\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Written prediction results to file: /content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv\n"]}],"source":["import os\n","\n","input_file_path = '/content/machine-learning/model-preparation/0.Combined-Dataset/stackoverflow_gson_cleaned.csv'\n","prediction_output_file = os.path.join(prediction_output_dir, 'stackoverflow_gson_test.csv')\n","\n","input_file_path = '/content/machine-learning/model-output/prediction_files/stackoverflow_gson_test_2023_03_24_161620.csv'\n","prediction_output_file = input_file_path\n","\n","\n","models = [#'cardiffnlp/twitter-roberta-base-sentiment-latest', \n","          #'/content/machine-learning/model-output/model_files/simcse--17.POME-StackOverflow--MLAuto-Emotion-D',\n","          # '/content/machine-learning/model-output/model_files/simcse--17.POME-StackOverflow--MLnone',\n","          # '/content/machine-learning/model-output/model_files/simcse--4.1.BinLin-AppReviews--MLAuto-Emotion-D',\n","          # '/content/machine-learning/model-output/model_files/simcse--4.1.BinLin-AppReviews--MLnone',\n","          # '/content/machine-learning/model-output/model_files/simcse--4.3.BinLin-SO--MLAuto-Emotion-D',\n","          # '/content/machine-learning/model-output/model_files/simcse--4.3.BinLin-SO--MLnone',\n","          # '/content/machine-learning/model-output/model_files/simcse--7.1.BERT4SentiSE-StackOverflow--MLAuto-Emotion-D',\n","          # '/content/machine-learning/model-output/model_files/simcse--7.1.BERT4SentiSE-StackOverflow--MLnone',\n","          '/content/machine-learning/model-output/model_files/simcse--4.3.BinLin-SO--ML--Auto-Emotion-D-T',\n","          '/content/machine-learning/model-output/model_files/simcse--5.Senti4SD-GoldStandard-EmotionPolarity--ML--Auto-Emotion-D-T',\n","          '/content/machine-learning/model-output/model_files/simcse--6.Opiner-StackOverflow--ML--Auto-Emotion-D-T',\n","          '/content/machine-learning/model-output/model_files/simcse--7.1.BERT4SentiSE-StackOverflow--ML--Auto-Emotion-D-T',\n","          ]\n","\n","for model_output_dir in models:\n","  print(input_file_path)\n","  print(prediction_output_file)\n","  print(model_output_dir)\n","\n","  classify(input_file_path, prediction_output_file, False, model_output_dir)\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP/6i+pbqBzDI5v1F+P2YoD","machine_shape":"hm","mount_file_id":"15uh0vWRUWeMMwQBKcJSHYQ3kPP6i64yJ","provenance":[{"file_id":"1oRNrrzdhTsHB9wD-zC65M5NLCo-Q4n8T","timestamp":1676910833180},{"file_id":"1z0-dgB6zPZes1GbV6jlhJqyUEx_EdAtt","timestamp":1676834345221},{"file_id":"1rKF4Pa0s0p0ZhNyQJMspH-kfenIR86Mz","timestamp":1676604392191},{"file_id":"1fWviCvRN9y-2sPTf1CgZB7vpO3Z2pkqW","timestamp":1676410371247},{"file_id":"1maTP-sXJRfxOGwj-tghNCoSCXmN5_Xgy","timestamp":1647872190559}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
